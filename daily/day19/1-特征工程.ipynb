{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、机器学习的DictVectorizer练习，文本分词，CountVectorizer, TfidfVectorizer练习\n",
    "# 2、归一化，标准化，缺失值插补练习\n",
    "# 3、练习特征预处理中的VarianceThreshold和PCA\n",
    "# 4、完成对数据集的data，target，DESCR，feature_names，target_names的理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T02:34:41.120877Z",
     "start_time": "2025-02-27T02:34:36.768216Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py\\Lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1 Dictionary 中含有字符串的时候（当成类别），如何做特征抽取"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "DictVectorizer\n",
    "一个字符串作为一个特征，所有字符对应一个表<br>\n",
    "如：北京  100  上海 010 ……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T02:47:38.594398Z",
     "start_time": "2025-02-27T02:47:38.590650Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse=True：\n",
      " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 6 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t100.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t60.0\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t30.0\n",
      "--------------------------------------------------\n",
      "Sparse=False：\n",
      " [[  0.   1.   0. 100.]\n",
      " [  1.   0.   0.  60.]\n",
      " [  0.   0.   1.  30.]]\n",
      "--------------------------------------------------\n",
      "特征名：\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "['city=上海' 'city=北京' 'city=深圳' 'temperature']\n",
      "--------------------------------------------------\n",
      "[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 60.0}, {'city=深圳': 1.0, 'temperature': 30.0}]\n",
      "[{'city=北京': 1.0, 'temperature': 100.0}, {'city=上海': 1.0, 'temperature': 60.0}, {'city=深圳': 1.0, 'temperature': 30.0}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dictvec():\n",
    "    \"\"\"\n",
    "    字典数据抽取\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # 初始化DictVectorizer，用于数据特征抽取\n",
    "    # sparse = True,输出的是每个不为零位置的坐标(稀疏矩阵) , 不推荐\n",
    "    dv1 = DictVectorizer(sparse=True)   \n",
    "    # sparse = False,输出的是密集矩阵, 推荐，默认\n",
    "    dv2 = DictVectorizer(sparse=False)  \n",
    "\n",
    "    # 三个样本，每个样本都是一个字典\n",
    "    dict = [{'city': '北京', 'temperature': 100},\n",
    "            {'city': '上海', 'temperature': 60},\n",
    "            {'city': '深圳', 'temperature': 30}]\n",
    "\n",
    "    data1 = dv1.fit_transform( dict )\n",
    "    data2 = dv2.fit_transform( dict )\n",
    "    print('Sparse=True：\\n',data1)\n",
    "    print('-' * 50)\n",
    "    print('Sparse=False：\\n',data2 )\n",
    "    print('-' * 50)\n",
    "    # 字典中的一些类别数据，分别进行转换成特征\n",
    "    print('特征名：')\n",
    "    print(dv1.get_feature_names_out()) #把每个特征名打印出来\n",
    "    print(dv2.get_feature_names_out()) #把每个特征名打印出来\n",
    "    print('-' * 50)\n",
    "    print(dv1.inverse_transform(data1))  #去看每个特征代表的含义，逆转回去,这里不重要\n",
    "    print(dv2.inverse_transform(data2))  #去看每个特征代表的含义，逆转回去,这里不重要\n",
    "    return None\n",
    "\n",
    "dictvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 一段英文文本如何变为数值类型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CountVectorizer\n",
    "根据规则(min和max)提取文本特征,向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T03:01:50.689059Z",
     "start_time": "2025-02-27T03:01:50.684550Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征词：\n",
      " ['is' 'life' 'python' 'short']\n",
      "--------------------------------------------------\n",
      "分词结果保存在稀疏矩阵内：\n",
      " <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 10 stored elements and shape (3, 4)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "稀疏矩阵转换为数组:\n",
      " [[1 2 1 1]\n",
      " [1 1 1 0]\n",
      " [1 1 0 1]]\n",
      "--------------------------------------------------\n",
      "[array(['life', 'is', 'short', 'python'], dtype='<U6'), array(['life', 'is', 'python'], dtype='<U6'), array(['life', 'is', 'short'], dtype='<U6')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def couvec():\n",
    "    # 实例化CountVectorizer\n",
    "    # max_df, min_df整数：min：至少出现几次的词才被统计。max：最多出现几次的词才被统计\n",
    "    # max_df, min_df小数(0-1之间的）：按在整个文本中最多/最少出现的百分比进行保留\n",
    "    # 默认会去除单个字母的单词，默认认为这个词对整个样本没有影响,认为其没有语义\n",
    "\n",
    "    vector = CountVectorizer(min_df=2)  # 至少出现2次的词才被统计\n",
    "\n",
    "    # 调用fit_transform输入并转换数据\n",
    "\n",
    "    res = vector.fit_transform(\n",
    "        [\"life is  short,i like python life\",\n",
    "         \"life is too long,i dislike python\",\n",
    "         \"life is short\"])\n",
    "\n",
    "    # 打印结果,把每个词都分离了\n",
    "    print('特征词：\\n',vector.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "\n",
    "    print('分词结果保存在稀疏矩阵内：\\n',res)\n",
    "    print('-' * 50)\n",
    "\n",
    "    print(type(res))\n",
    "    # 对照feature_names，标记每个词出现的次数\n",
    "    print('-' * 50)\n",
    "\n",
    "    #稀疏矩阵转换为数组\n",
    "    print('稀疏矩阵转换为数组:\\n',res.toarray())  \n",
    "    print('-' * 50)\n",
    "\n",
    "    #拿每个样本里的特征进行显示\n",
    "    print(vector.inverse_transform(res)) #不重要\n",
    "\n",
    "\n",
    "couvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 一段汉字文本如何数值化，对于汉字不能用空格来分割"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1、继续使用CountVectorizer,提取文本特征,向量化\n",
    "2、使用jieba分词,再使用CountVectorizer取文本特征,向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T03:08:47.503161Z",
     "start_time": "2025-02-27T03:08:47.499133Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征词：  ['python' '不用' '人生漫长' '人生苦短' '我喜欢']\n",
      "--------------------------------------------------\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 6 stored elements and shape (2, 5)>\n",
      "  Coords\tValues\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "--------------------------------------------------\n",
      "[[2 0 0 1 1]\n",
      " [1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def countvec():\n",
    "    \"\"\"\n",
    "    使用CountVectorizer对文本进行特征值化 \n",
    "    $注意 ： 和单个字母一样，单个汉字单个字母不统计，因为单个汉字字母没有意义   \n",
    "    \"\"\"\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    data = cv.fit_transform([\"人生苦短，我喜欢 python python\", \"人生漫长，不用 python\"])\n",
    "\n",
    "    print('特征词： ',cv.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "\n",
    "    #默认会变成稀疏矩阵存储，只记录非零位置\n",
    "    print(data)  \n",
    "    print('-' * 50)\n",
    "\n",
    "    print(data.toarray())\n",
    "    return None\n",
    "\n",
    "\n",
    "countvec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 掌握如何对中文进行分词(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T03:18:49.541288Z",
     "start_time": "2025-02-27T03:18:49.535767Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\19942\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba分词器产生的结果类型是： <class 'generator'>\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.618 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "分词好的文本：\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。\n",
      "我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。\n",
      "如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "--------------------------------------------------\n",
      "特征词： \n",
      " ['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "\n",
      "将稀疏矩阵变成密集矩阵：\n",
      " [[0 0 1 0 0 0 2 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 2 0 1 0 2 1 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 1 1]\n",
      " [1 1 0 0 4 3 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 2 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cutword():\n",
    "    \"\"\"\n",
    "    通过 jieba.cut 对中文进行分词\n",
    "    \"\"\"\n",
    "    con1 = jieba.cut(\"今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\")\n",
    "\n",
    "    con2 = jieba.cut(\"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\")\n",
    "\n",
    "    con3 = jieba.cut(\n",
    "        \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\")\n",
    "\n",
    "    # 转换成列表\n",
    "    print('jieba分词器产生的结果类型是：',type(con1))\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 把生成器转换成列表\n",
    "    content1 = list(con1)\n",
    "    content2 = list(con2)\n",
    "    content3 = list(con3)\n",
    "    print(content1)\n",
    "    print(content2)\n",
    "    print(content3)\n",
    "\n",
    "    # 把列表转换成字符串,每个词之间用空格隔开\n",
    "    print('-' * 50)\n",
    "    c1 = ' '.join(content1)\n",
    "    c2 = ' '.join(content2)\n",
    "    c3 = ' '.join(content3)\n",
    "    return c1, c2, c3\n",
    "\n",
    "\n",
    "def chinese_vec():\n",
    "    \"\"\"\n",
    "    把jieba分好的词,对中文进行特征值化\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()  #jieba分词好的中文文本\n",
    "    print('-' * 50)\n",
    "\n",
    "    print('分词好的文本：')\n",
    "    print(c1)  \n",
    "    print(c2)\n",
    "    print(c3)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 使用CountVectorizer对中文进行特征值化\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    data = cv.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print('特征词： \\n',cv.get_feature_names_out())  #把处理好后的特征名称打印出来\n",
    "\n",
    "    print('\\n将稀疏矩阵变成密集矩阵：\\n',data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# cutword()\n",
    "chinese_vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1.4 tf-idf\n",
    "用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度<br>\n",
    "TF-IDF 越高，**说明这个词在这篇文档中很重要，但在其他文档中很少见。而不是所有文档都很常见的（比如'的'）**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tf : 某一个单词出现的词频\n",
    "tf = 词频 / 该文档所有词的词频\n",
    "\n",
    "idf : 逆文档频率，表示某词在所有文档中是否罕见\n",
    "idf = log( 语料库的文本总数+1 / 包含该词的文本数 + 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:15:43.673947Z",
     "start_time": "2025-02-27T06:15:43.668440Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba分词器产生的结果类型是： <class 'generator'>\n",
      "--------------------------------------------------\n",
      "['今天', '很', '残酷', '，', '明天', '更', '残酷', '，', '后天', '很', '美好', '，', '但', '绝对', '大部分', '是', '死', '在', '明天', '晚上', '，', '所以', '每个', '人', '不要', '放弃', '今天', '。']\n",
      "['我们', '看到', '的', '从', '很', '远', '星系', '来', '的', '光是在', '几百万年', '之前', '发出', '的', '，', '这样', '当', '我们', '看到', '宇宙', '时', '，', '我们', '是', '在', '看', '它', '的', '过去', '。']\n",
      "['如果', '只用', '一种', '方式', '了解', '某样', '事物', '，', '你', '就', '不会', '真正', '了解', '它', '。', '了解', '事物', '真正', '含义', '的', '秘密', '取决于', '如何', '将', '其', '与', '我们', '所', '了解', '的', '事物', '相', '联系', '。']\n",
      "--------------------------------------------------\n",
      "今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。 我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。 如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。\n",
      "['一种' '不会' '不要' '之前' '了解' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义'\n",
      " '大部分' '如何' '如果' '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个'\n",
      " '看到' '真正' '秘密' '绝对' '美好' '联系' '过去' '这样']\n",
      "--------------------------------------------------\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "--------------------------------------------------\n",
      "[[0.         0.         0.21821789 0.         0.         0.\n",
      "  0.43643578 0.         0.         0.         0.         0.\n",
      "  0.21821789 0.         0.21821789 0.         0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.43643578 0.\n",
      "  0.21821789 0.         0.43643578 0.21821789 0.         0.\n",
      "  0.         0.21821789 0.21821789 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.2410822  0.         0.\n",
      "  0.         0.2410822  0.2410822  0.2410822  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2410822\n",
      "  0.55004769 0.         0.         0.         0.         0.2410822\n",
      "  0.         0.         0.         0.         0.48216441 0.\n",
      "  0.         0.         0.         0.         0.2410822  0.2410822 ]\n",
      " [0.15698297 0.15698297 0.         0.         0.62793188 0.47094891\n",
      "  0.         0.         0.         0.         0.15698297 0.15698297\n",
      "  0.         0.15698297 0.         0.15698297 0.15698297 0.\n",
      "  0.1193896  0.         0.         0.15698297 0.         0.\n",
      "  0.         0.15698297 0.         0.         0.         0.31396594\n",
      "  0.15698297 0.         0.         0.15698297 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 规范{'l1'，'l2'}，默认='l2'\n",
    "# 每个输出行都有单位范数，或者：\n",
    "#\n",
    "# 'l2'：向量元素的平方和为 1。当应用 l2 范数时，两个向量之间的余弦相似度是它们的点积。\n",
    "#\n",
    "# 'l1'：向量元素的绝对值之和为 1。参见preprocessing.normalize。\n",
    "\n",
    "# smooth_idf布尔值，默认 = True\n",
    "# 通过在文档频率上加一来平滑 idf 权重，就好像看到一个额外的文档包含集合中的每个术语恰好一次。防止零分裂。\n",
    "# 比如训练集中有某个词，测试集中没有，就是生僻词，就会造成n(x)分母为零，log(n/n(x)),从而出现零分裂\n",
    "\n",
    "def tfidf_vec():\n",
    "    \"\"\"\n",
    "    中文特征值化,计算tfidf值\n",
    "    \"\"\"\n",
    "    c1, c2, c3 = cutword()\n",
    "\n",
    "    print(c1, c2, c3)\n",
    "    # print(type([c1, c2, c3]))\n",
    "\n",
    "    # 计算tfidf值\n",
    "    tf = TfidfVectorizer(smooth_idf=True)\n",
    "\n",
    "    data = tf.fit_transform([c1, c2, c3])\n",
    "\n",
    "    print(tf.get_feature_names_out())\n",
    "    print('-' * 50)\n",
    "    print(type(data))\n",
    "    print('-' * 50)\n",
    "    print(data.toarray())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "tfidf_vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 特征处理，不同的特征拉到到同一个量纲"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1. 归一化\n",
    "2. 标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:32:08.076338Z",
     "start_time": "2025-02-27T06:32:08.070326Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练：\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.         1.         1.         0.83333333]\n",
      " [0.5        0.5        0.6        1.        ]]\n",
      "--------------------------------------------------\n",
      "测试：\n",
      " [[-1.96666667  0.         -1.4        -6.        ]\n",
      " [-1.8         1.5        -0.4        -5.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def mm():\n",
    "    \"\"\"\n",
    "    归一化处理\n",
    "    \"\"\"\n",
    "    # 归一化缺点 容易受极值的影响\n",
    "    \n",
    "    # 用MinmaxScaler设置归一化范围，通过feature_range参数设置\n",
    "    mm = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # fit_transfrom() 训练数据\n",
    "    train_data = mm.fit_transform([[90, 2, 10, 40], [60, 4, 15, 45], [75, 3, 13, 46]])\n",
    "    print('归一化训练：\\n', train_data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # transform() 测试数据\n",
    "    test_data = mm.transform([[1, 2, 3, 4], [6, 5, 8, 7]]) #测试集\n",
    "    print('归一化测试：\\n', test_data)\n",
    "\n",
    "    return None\n",
    "    #transform和fit_transform不同是，transform用于测试集，而且不会重新找最小值和最大值\n",
    "\n",
    "\n",
    "mm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:30:10.607819Z",
     "start_time": "2025-02-27T06:30:10.603314Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9666666666666666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - 60) / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:41:32.347652Z",
     "start_time": "2025-02-27T06:41:32.333963Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准化训练：\n",
      " [[-1.06904497 -1.35873244  0.98058068]\n",
      " [-0.26726124  0.33968311  0.39223227]\n",
      " [ 1.33630621  1.01904933 -1.37281295]]\n",
      "--------------------------------------------------\n",
      "训练数据的均值：\n",
      " [2.33333333 3.         1.33333333]\n",
      "--------------------------------------------------\n",
      "训练数据的方差：\n",
      " [1.55555556 8.66666667 2.88888889]\n",
      "训练数据的样本数：\n",
      " 3\n",
      "--------------------------------------------------\n",
      "数据类型：\n",
      " <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def stand():\n",
    "    \"\"\"\n",
    "    标准化,$注意： 不是标准正太分布,只均值为0,方差为1的分布\n",
    "    $注意： 标准化，是针对每一列进行标准化\n",
    "    \"\"\"\n",
    "    # 引入StandardScaler，用于标准化处理\n",
    "    std = StandardScaler()\n",
    "\n",
    "    # fit_transform() 训练数据\n",
    "    data = std.fit_transform([[1., -1., 3.],\n",
    "                              [2., 4., 2.],\n",
    "                              [4., 6., -1.]])\n",
    "\n",
    "    print('标准化训练：\\n', data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    print('训练数据的均值：\\n', std.mean_)\n",
    "    print('-' * 50)\n",
    "\n",
    "    print('训练数据的方差：\\n', std.var_)  #方差\n",
    "    print('训练数据的样本数：\\n', std.n_samples_seen_)  # 样本数\n",
    "\n",
    "    print('-' * 50)\n",
    "    print('fit_transform() 训练后返回的数据类型：\\n', type(data))\n",
    "\n",
    "\n",
    "data = stand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T06:08:50.094350Z",
     "start_time": "2025-01-09T06:08:50.091180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5555556666666668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如何求方差 （x-mean）**2 / n\n",
    "(np.square(1 - 2.333) + np.square(2 - 2.333) + np.square(4 - 2.333)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:48:59.683168Z",
     "start_time": "2025-02-27T06:48:59.679117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算每一列，标准化后数据的均值，验证是否接近0\n",
    "(-1.06904497 + -0.26726124 + 1.33630621) / 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 验证已经标准化过的数据再次标准化后，均值应该为0，方差应该为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:54:53.193357Z",
     "start_time": "2025-02-27T06:54:53.189342Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.48029737e-16  7.40148683e-17  7.40148683e-17]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个新的StandardScaler对象用于标准化\n",
    "std1 = StandardScaler()\n",
    "\n",
    "# 对已经标准化过的数据进行再次标准化，验证结果\n",
    "data1 = std1.fit_transform(data)\n",
    "\n",
    "# 均值\n",
    "print(std1.mean_)\n",
    "# 方差\n",
    "print(std1.var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### transform和fit_transform不同是，transform用于测试集，而且不会重新找最小值和最大值,不会重新计算均值方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T06:59:49.896513Z",
     "start_time": "2025-02-27T06:59:49.881718Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.        ]\n",
      " [3.66666667 3.        ]\n",
      " [7.         6.        ]\n",
      " [3.         2.        ]]\n"
     ]
    }
   ],
   "source": [
    "#下面是填补缺失值，针对删除，可以用pd和np\n",
    "def im():\n",
    "    \"\"\"\n",
    "    缺失值处理\n",
    "    SimpleImputer( missing_values=指定谁是缺失值, strategy=指定填补策略)\n",
    "    \"\"\"\n",
    "    # np.nan替换为均值\n",
    "    # mean, median, most_frequent(众数), constant\n",
    "    im = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "    # 创建一个包含缺失值的数组\n",
    "    data = im.fit_transform([[1, 2], [np.nan, 3], [7, 6], [3, 2]])\n",
    "\n",
    "    # 打印填补后的数据\n",
    "    print(data)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "im()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4 降维（拟合）\n",
    "降维就是特征数变少<br>\n",
    "降维可以提高模型训练速度（特征变少）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方差阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:38:02.437188Z",
     "start_time": "2025-02-27T07:38:02.433791Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "删除了低于10%方差列的训练结果：\n",
      " [[2 0]\n",
      " [1 4]\n",
      " [1 1]]\n",
      "--------------------------------------------------\n",
      "保留下来的列编号 [1 2]\n"
     ]
    }
   ],
   "source": [
    "def var():\n",
    "    \"\"\"\n",
    "    特征选择-删除低方差的特征\n",
    "    \"\"\"\n",
    "    #默认只删除方差为0,threshold是方差阈值，删除比这个值小的那些特征， threshold：小数\n",
    "    var = VarianceThreshold(threshold=0.1)\n",
    "\n",
    "    # 导入数据训练\n",
    "    data = var.fit_transform([[0, 2, 0, 3],\n",
    "                              [0, 1, 4, 3],\n",
    "                              [0, 1, 1, 3]])\n",
    "\n",
    "    print('删除了低于10%方差列的训练结果：\\n', data)\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 获取符合条件的特征列编号\n",
    "    print('保留下来的列编号 %s' % var.get_support(True))\n",
    "    return None\n",
    "\n",
    "\n",
    "var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarianceThreshold用于特征选择，去除方差低于阈值的特征\n",
    "print(\"\\nVarianceThreshold示例:\")\n",
    "\n",
    "# 创建一个示例数据矩阵\n",
    "X_variance = np.array([\n",
    "    [0, 2, 0, 3],\n",
    "    [0, 1, 4, 3],\n",
    "    [0, 1, 1, 3]\n",
    "])\n",
    "print(\"原始数据矩阵:\\n\", X_variance)\n",
    "\n",
    "# 计算每个特征的方差\n",
    "feature_variances = np.var(X_variance, axis=0)\n",
    "print(\"\\n各特征的方差:\", feature_variances)\n",
    "\n",
    "# 使用方差阈值为0.8进行特征选择\n",
    "# 这将移除方差小于0.8的特征\n",
    "selector = VarianceThreshold(threshold=0.8)\n",
    "X_selected = selector.fit_transform(X_variance)\n",
    "\n",
    "# 显示保留的特征索引\n",
    "print(\"\\n保留的特征索引:\", selector.get_support(indices=True))\n",
    "print(\"保留的特征方差:\", feature_variances[selector.get_support()])\n",
    "print(\"\\n特征选择后的数据矩阵:\\n\", X_selected)\n",
    "\n",
    "# 尝试不同的阈值\n",
    "selector_low = VarianceThreshold(threshold=0.1)\n",
    "X_selected_low = selector_low.fit_transform(X_variance)\n",
    "print(\"\\n阈值为0.1时保留的特征索引:\", selector_low.get_support(indices=True))\n",
    "print(\"阈值为0.1时特征选择后的数据矩阵:\\n\", X_selected_low)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:50:02.217367Z",
     "start_time": "2025-02-27T07:50:02.212451Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最初数据每一列的方差和：\n",
      " 29.333333333333336\n",
      "--------------------------------------------------\n",
      "PCA训练后的数据：\n",
      " [[-1.28620952e-15  3.82970843e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]]\n",
      "--------------------------------------------------\n",
      "PCA训练后的数据类型：\n",
      " <class 'numpy.ndarray'>\n",
      "--------------------------------------------------\n",
      "PCA训练后的数据每一列的方差和：\n",
      " [22.          7.33333333]\n",
      "--------------------------------------------------\n",
      "PCA训练后的数据每一列的方差占总方差的比例：\n",
      " [0.75 0.25]\n",
      "--------------------------------------------------\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def pca():\n",
    "    \"\"\"\n",
    "    主成分分析进行特征降维\n",
    "    \"\"\"\n",
    "    # n_ components:小数 0~1 业界选择 90~95% 。建议 90%\n",
    "\n",
    "    # 当n_components的值为0到1之间的浮点数时，表示我们希望保留的主成分解释的方差比例。方差比例是指 得到输出的每一列的方差值和除以原有数据方差之和。\n",
    "    # n_components如果是整数   代表要把列（特征）降低到多少列\n",
    "\n",
    "    # 原始数据方差\n",
    "    original_value = np.array([[2, 8, 4, 5],\n",
    "                               [6, 3, 0, 8],\n",
    "                               [5, 4, 9, 1]])\n",
    "\n",
    "    print('最初数据每一列的方差和：\\n', np.var(original_value, axis=0).sum())  \n",
    "    print('-' * 50)\n",
    "\n",
    "    # 创建PCA对象，当前几个主成分的方差贡献率和=90%，保留这几个主成分，而\n",
    "    # 小数的优点：不需要手动指定降低到多少列\n",
    "    pca = PCA(n_components=0.9)\n",
    "\n",
    "    # 用PCA 训练数据\n",
    "    data = pca.fit_transform(original_value)\n",
    "\n",
    "    print('PCA训练后的数据：\\n', data)\n",
    "    print('-' * 50)\n",
    "    print('PCA训练后的数据类型：\\n', type(data))\n",
    "    print('-' * 50)\n",
    "\n",
    "    #计算data的方差和\n",
    "    print('PCA训练后的数据每一列的方差和：\\n', np.var(data, axis=0))\n",
    "    print('-' * 50)\n",
    "\n",
    "    # 计算data的方差占总方差的比例\n",
    "  \n",
    "    # 第一个主成分解释了75%的方差\n",
    "    # 75%不够90%，所以需要第二个主成分，二个主成分解释了25%的方差，\n",
    "    print('PCA训练后的数据每一列的方差占总方差的比例：\\n', pca.explained_variance_ratio_)\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # 计算data的方差占总方差的比例,加起来是1\n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T07:48:46.807425Z",
     "start_time": "2025-02-27T07:48:46.804261Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA训练后方差应该非常接近原data的方差\n",
    "29.333333333333332 / 29.333333333333336"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
